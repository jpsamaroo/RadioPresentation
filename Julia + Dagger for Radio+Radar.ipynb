{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c788130e",
   "metadata": {},
   "source": [
    "# Introduction to Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d257bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               \u001b[1m\u001b[32m_\u001b[0m\n",
      "   \u001b[1m\u001b[34m_\u001b[0m       \u001b[0m_\u001b[0m \u001b[1m\u001b[31m_\u001b[1m\u001b[32m(_)\u001b[1m\u001b[35m_\u001b[0m     |  Documentation: https://docs.julialang.org\n",
      "  \u001b[1m\u001b[34m(_)\u001b[0m     | \u001b[1m\u001b[31m(_)\u001b[0m \u001b[1m\u001b[35m(_)\u001b[0m    |\n",
      "   \u001b[0m_ _   _| |_  __ _\u001b[0m   |  Type \"?\" for help, \"]?\" for Pkg help.\n",
      "  \u001b[0m| | | | | | |/ _` |\u001b[0m  |\n",
      "  \u001b[0m| | |_| | | | (_| |\u001b[0m  |  Version 1.9.0 (2023-05-14)\n",
      " \u001b[0m_/ |\\__'_|_|_|\\__'_|\u001b[0m  |  backports-release-1.9/222a9272bd (fork: 328 commits, 217 days)\n",
      "\u001b[0m|__/\u001b[0m                   |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Base.banner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450d999",
   "metadata": {},
   "source": [
    "### Julia is a dynamic language..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9564f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_sum(1000, 1)::Int64 = 1000\n",
      "simple_sum(1000, 1.0)::Float64 = 1000.0\n",
      "simple_sum(1000, 1.0 + 1im)::ComplexF64 = 1000.0 + 1000.0im\n"
     ]
    }
   ],
   "source": [
    "function simple_sum(N, inc::T) where T\n",
    "    x = zero(T) # Creates a 0 value of type T\n",
    "    for i in 1:N\n",
    "        x += inc\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "@show simple_sum(1000, 1)::Int64;\n",
    "@show simple_sum(1000, 1.0)::Float64;\n",
    "@show simple_sum(1000, 1.0 + 1im)::ComplexF64;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e716f7d",
   "metadata": {},
   "source": [
    "### ...but it's also a compiled language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "011ce55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[0m.text\n",
      "\t\u001b[0m.file\t\u001b[0m\"simple_sum\"\n",
      "\t\u001b[0m.globl\t\u001b[0mjulia_simple_sum_1237           \u001b[90m# -- Begin function julia_simple_sum_1237\u001b[39m\n",
      "\t\u001b[0m.p2align\t\u001b[33m4\u001b[39m\u001b[0m, \u001b[33m0x90\u001b[39m\n",
      "\t\u001b[0m.type\t\u001b[0mjulia_simple_sum_1237\u001b[0m,\u001b[0m@function\n",
      "\u001b[91mjulia_simple_sum_1237:\u001b[39m                  \u001b[90m# @julia_simple_sum_1237\u001b[39m\n",
      "\t\u001b[0m.cfi_startproc\n",
      "\u001b[90m# %bb.0:                                # %top\u001b[39m\n",
      "\t\u001b[96m\u001b[1mpushq\u001b[22m\u001b[39m\t\u001b[0m%rbp\n",
      "\t\u001b[0m.cfi_def_cfa_offset \u001b[33m16\u001b[39m\n",
      "\t\u001b[0m.cfi_offset \u001b[0m%rbp\u001b[0m, \u001b[33m-16\u001b[39m\n",
      "\t\u001b[96m\u001b[1mmovq\u001b[22m\u001b[39m\t\u001b[0m%rsp\u001b[0m, \u001b[0m%rbp\n",
      "\t\u001b[0m.cfi_def_cfa_register \u001b[0m%rbp\n",
      "\t\u001b[96m\u001b[1mimulq\u001b[22m\u001b[39m\t\u001b[0m%rdi\u001b[0m, \u001b[0m%rsi\n",
      "\t\u001b[96m\u001b[1mxorl\u001b[22m\u001b[39m\t\u001b[0m%eax\u001b[0m, \u001b[0m%eax\n",
      "\t\u001b[96m\u001b[1mtestq\u001b[22m\u001b[39m\t\u001b[0m%rdi\u001b[0m, \u001b[0m%rdi\n",
      "\t\u001b[96m\u001b[1mcmovgq\u001b[22m\u001b[39m\t\u001b[0m%rsi\u001b[0m, \u001b[0m%rax\n",
      "\t\u001b[96m\u001b[1mpopq\u001b[22m\u001b[39m\t\u001b[0m%rbp\n",
      "\t\u001b[0m.cfi_def_cfa \u001b[0m%rsp\u001b[0m, \u001b[33m8\u001b[39m\n",
      "\t\u001b[96m\u001b[1mretq\u001b[22m\u001b[39m\n",
      "\u001b[91m.Lfunc_end0:\u001b[39m\n",
      "\t\u001b[0m.size\t\u001b[0mjulia_simple_sum_1237\u001b[0m, \u001b[0m.Lfunc_end0-julia_simple_sum_1237\n",
      "\t\u001b[0m.cfi_endproc\n",
      "                                        \u001b[90m# -- End function\u001b[39m\n",
      "\t\u001b[0m.section\t\u001b[0m\".note.GNU-stack\"\u001b[0m,\u001b[0m\"\"\u001b[0m,\u001b[0m@progbits\n"
     ]
    }
   ],
   "source": [
    "@code_native debuginfo=:none simple_sum(1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d01a4",
   "metadata": {},
   "source": [
    "### Julia has powerful abstractions for multitasking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289f39ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from 3\n",
      "Hello from 8\n",
      "Hello from 9\n",
      "Hello from 6\n",
      "Hello from 10\n",
      "Hello from 1\n",
      "Hello from 7\n",
      "Hello from 4\n",
      "Hello from 5\n",
      "Hello from 2\n",
      "All hello's have been said!\n"
     ]
    }
   ],
   "source": [
    "@sync for i in 1:10\n",
    "    Threads.@spawn println(\"Hello from $i\")\n",
    "end\n",
    "wait(Threads.@spawn println(\"All hello's have been said!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e504343",
   "metadata": {},
   "source": [
    "### ...which automatically run across threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63726a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from thread 3\n",
      "Hello from thread 5\n",
      "Hello from thread 1\n",
      "Hello from thread 6\n",
      "Hello from thread 2\n",
      "Hello from thread 4\n"
     ]
    }
   ],
   "source": [
    "@sync for i in 1:6\n",
    "    Threads.@spawn println(\"Hello from thread $(Threads.threadid())\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e82ae7",
   "metadata": {},
   "source": [
    "### Julia has support for multiprocessing (distributed computing) too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0904d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributed\n",
    "ENV[\"JULIA_PROJECT\"] = pwd() # Annoying but sometimes required\n",
    "w2, w3 = addprocs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772f8b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\tHello from worker 2\n"
     ]
    }
   ],
   "source": [
    "@spawnat w2 println(\"Hello from worker $(myid())\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54c2da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\tHello from worker 3\n"
     ]
    }
   ],
   "source": [
    "@spawnat w3 println(\"Hello from worker $(myid())\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070223e8",
   "metadata": {},
   "source": [
    "### But how do we tie these capabilities together?\n",
    "\n",
    "Unfortunately, Julia's multitasking system doesn't have any built-in mechanism to span multiple servers; a task spawned on a server, stays on that server. Making multithreading and multiprocessing cooperate is quite cumbersome, and requires the implementation of all kinds of data structures, task runtimes, resource management, error reporting, etc.\n",
    "\n",
    "Is there a solution to be had? Yes!\n",
    "\n",
    "## Intro to Dagger\n",
    "\n",
    "Dagger.jl is a Julia library which implements all of those annoying details, and makes it possible to use multitasking across multiple servers. At its core, Dagger uses a Directed Acyclic Graph (DAG) to model the user's computations and data dependencies, and has a set of cooperating schedulers to assign, execute, and manage tasks and their associated data.\n",
    "\n",
    "Dagger has a number of very useful features that makes parallel programming productive. Most of these features rely on Julia's built-in features or on other Julia libraries, with Dagger acting as a wrapper around them. This might sound like unnecessary work, but it really makes Dagger into a one-stop shop for parallelism of all sorts.\n",
    "\n",
    "For example, let's see how multithreading and multiprocessing can be used in Dagger. Here's some simple code that automatically runs tasks on any CPU thread of any server in our Julia cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0f5a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world from worker 1, thread 2\n",
      "      From worker 2:\tHello world from worker 2, thread 1\n",
      "      From worker 2:\tHello world from worker 2, thread 3\n",
      "      From worker 2:\tHello world from worker 2, thread 4\n",
      "      From worker 2:\tHello world from worker 2, thread 2\n",
      "      From worker 2:\tHello world from worker 2, thread 5\n",
      "      From worker 2:\tHello world from worker 2, thread 3\n",
      "      From worker 3:\tHello world from worker 3, thread 2\n",
      "      From worker 3:\tHello world from worker 3, thread 3\n",
      "      From worker 3:\tHello world from worker 3, thread 5\n"
     ]
    }
   ],
   "source": [
    "using Dagger\n",
    "\n",
    "f = ()->println(\"Hello world from worker $(myid()), thread $(Threads.threadid())\")\n",
    "\n",
    "@sync for i in 1:10\n",
    "    Dagger.@spawn f()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd6f7e",
   "metadata": {},
   "source": [
    "Conveniently, we didn't have to tell Dagger where to run our tasks, it just ran them on the first available processing resource that it had access to. We can keep throwing tasks onto Dagger, and it'll do its best to spread the load evenly.\n",
    "\n",
    "Ok, so Dagger will load balance, but what happens if we need to move data between our tasks? Wouldn't load balancing cause lots of wasted data movement? Not to fear, Dagger is smart!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b74dba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for 1 on worker 1, thread 4\n",
      "Generating for 2 on worker 1, thread 6\n",
      "Generating for 8 on worker 1, thread 2\n",
      "      From worker 2:\tGenerating for 9 on worker 2, thread 6\n",
      "      From worker 2:\tGenerating for 5 on worker 2, thread 3\n",
      "      From worker 2:\tGenerating for 7 on worker 2, thread 4\n",
      "      From worker 3:\tGenerating for 4 on worker 3, thread 5\n",
      "      From worker 3:\tGenerating for 10 on worker 3, thread 6\n",
      "      From worker 3:\tGenerating for 6 on worker 3, thread 1\n",
      "      From worker 3:\tGenerating for 3 on worker 3, thread 4\n",
      "Summing for 1 on worker 1 (was 1), thread 4\n",
      "Summing for 8 on worker 1 (was 1), thread 3\n",
      "Summing for 2 on worker 1 (was 1), thread 5\n",
      "      From worker 2:\tSumming for 9 on worker 2 (was 2), thread 2\n",
      "      From worker 2:\tSumming for 7 on worker 2 (was 2), thread 5\n",
      "      From worker 2:\tSumming for 5 on worker 2 (was 2), thread 4\n",
      "      From worker 3:\tSumming for 6 on worker 3 (was 3), thread 3\n",
      "      From worker 3:\tSumming for 4 on worker 3 (was 3), thread 2\n",
      "      From worker 3:\tSumming for 3 on worker 3 (was 3), thread 1\n",
      "      From worker 3:\tSumming for 10 on worker 3 (was 3), thread 6\n"
     ]
    }
   ],
   "source": [
    "g = (i) -> begin\n",
    "    println(\"Generating for $i on worker $(myid()), thread $(Threads.threadid())\")\n",
    "    return (myid(), rand(200, 200))\n",
    "end\n",
    "h = (i, (id, X)) -> begin\n",
    "    println(\"Summing for $i on worker $(myid()) (was $id), thread $(Threads.threadid())\")\n",
    "    sum(X)\n",
    "end\n",
    "\n",
    "@sync for i in 1:10\n",
    "    t = Dagger.@spawn g(i)\n",
    "    Dagger.@spawn h(i, t)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97637212",
   "metadata": {},
   "source": [
    "Dagger models the costs of data movement when assigning tasks to compute resources, which makes data-intensive computations run efficiently while still being load balanced (and as we can see, Dagger still freely picks different threads on the same worker, because there's no data movement involved).\n",
    "\n",
    "We can also see above how data typically moves within Dagger: data generated by one task can be trivially consumed as the argument to another task. Dagger will move the data between servers as necessary to ensure that tasks have all the data they need.\n",
    "\n",
    "What if we have some data that we can't or don't want to move? Maybe the data has some internal pointers, or it's a mutable object that can't be serialized between processes. Easy enough, `Dagger.@mutable` to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74670bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Tell Dagger that our data should be treated as mutable data, local to this server\n",
    "mydata = Dagger.@mutable (;tsk=current_task(), ref=Threads.Atomic{Int}(0))\n",
    "mydata::Dagger.Chunk\n",
    "\n",
    "j = data -> begin\n",
    "    @assert myid() == 1 # We're locked to worker 1 (this server) because of `mydata`\n",
    "    @assert !istaskdone(data.tsk) # Tasks can't be serialized\n",
    "    Threads.atomic_add!(data.ref, 1) # Serializing this would be undefined behavior\n",
    "end\n",
    "\n",
    "@sync for i in 1:10\n",
    "    Dagger.@spawn j(mydata)\n",
    "end\n",
    "println(fetch(mydata).ref[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f43cb",
   "metadata": {},
   "source": [
    "So we can lock data to a given server, cool enough. But that just gives us *one* piece of data; what if we want to have some data on *all* of our servers, like a cache or buffer? Not a problem; `Dagger.@shard` was made for exactly this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cb0e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2 on worker 1, thread 5\n",
      "Adding 1 on worker 1, thread 3\n",
      "Adding 3 on worker 1, thread 6\n",
      "Adding 4 on worker 1, thread 4\n",
      "      From worker 2:\tAdding 7 on worker 2, thread 3\n",
      "      From worker 3:\tAdding 6 on worker 3, thread 2\n",
      "      From worker 2:\tAdding 8 on worker 2, thread 4\n",
      "      From worker 3:\tAdding 9 on worker 3, thread 1\n",
      "      From worker 3:\tAdding 10 on worker 3, thread 4\n",
      "      From worker 3:\tAdding 5 on worker 3, thread 6\n",
      "55\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "mycache = Dagger.@shard Threads.Atomic{Int}(0)\n",
    "\n",
    "k = (cache, i) -> begin\n",
    "    println(\"Adding $i on worker $(myid()), thread $(Threads.threadid())\")\n",
    "    Threads.atomic_add!(cache, i)\n",
    "end\n",
    "\n",
    "@sync for i in 1:10\n",
    "    Dagger.@spawn k(mycache, i)\n",
    "end\n",
    "\n",
    "println(sum(map(fetch, map(c->c[], mycache))))\n",
    "println(sum(1:10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc784812",
   "metadata": {},
   "source": [
    "#### Future Work: Streaming\n",
    "\n",
    "If Dagger is to become a useful component of EdgeRF, there are still features missing that we'll need to have. The first feature that we'll definitely need is streaming data support. Right now, Dagger tasks have a limited lifetime - they run just once, and then are done forever. This is a super inefficient model for processing an arbitrary or inifite amount of data coming in from sensors, the network, or storage - we'd be generating tons of little tasks that exist only to execute a little bit of code!\n",
    "\n",
    "There's an obvious solution here: just pass a `Channel` or some other asynchronous buffer to a task and either `take!` from it or `put!` to it in a loop! And this is exactly what I think we should do, but with more explicit support from Dagger to ensure that this works seamlessly across the network and in the face of network or device failures (resets, packet loss, etc.).\n",
    "\n",
    "#### Future Work: Decentralization\n",
    "\n",
    "Similarly, running Dagger at the edge implies that there will need to be some amount of decentralized control. Sure, we might want a central planner to coordinate things, but to build a truly fault tolerant system, individual servers will need to be able to make some decisions for themselves. They should be able to keep running local streaming tasks, writing sensor data to disk, and communicate with any other reachable servers as necessary.\n",
    "\n",
    "Dagger currently expects to have a single central scheduler that spawns and manages all other servers. It should be possible to relax this requirement by letting each server launch and manage its own tasks. Especially in the context of streaming tasks, it should be possible to allow Dagger to gracefully and temporarily pause only tasks which require communication across the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351ee37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
